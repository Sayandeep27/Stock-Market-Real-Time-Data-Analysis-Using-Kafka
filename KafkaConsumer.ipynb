{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV5yTipYyKv65FCGn8yFPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayandeep27/Stock-Market-Real-Time-Data-Analysis-Using-Kafka/blob/main/KafkaConsumer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj5dAr0khbV-"
      },
      "outputs": [],
      "source": [
        "from kafka import KafkaConsumer  # Imports the KafkaConsumer class from the kafka library. KafkaConsumer is used to read (consume) messages from Kafka topics.\n",
        "from time import sleep    # Imports the sleep function from the time library. sleep is used to pause the execution of the program for a specified duration.\n",
        "from json import dumps,loads   # Imports the dumps and loads functions from the json library. dumps converts Python objects to JSON format, and loads converts JSON strings back to Python objects\n",
        "import json   # Imports the entire json library. This library helps in working with JSON data, like converting between JSON and Python objects.\n",
        "from s3fs import S3FileSystem  # Imports the S3FileSystem class from the s3fs library. S3FileSystem is used to interact with Amazon S3 storage, allowing you to read from and write to S3 buckets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consumer = KafkaConsumer(\n",
        "    'demo_test',\n",
        "     bootstrap_servers=[':9092'], #add your IP here\n",
        "\n",
        "    value_deserializer=lambda x: loads(x.decode('utf-8')))"
      ],
      "metadata": {
        "id": "VsoeW8XohiMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "consumer = KafkaConsumer(\n",
        "This line creates a Kafka consumer object named consumer to read messages from Kafka.\n",
        "\n",
        "'demo_test',\n",
        "This specifies the Kafka topic ('demo_test') from which the consumer will read messages.\n",
        "\n",
        "bootstrap_servers=[':9092'],\n",
        "This specifies the address of the Kafka server. Replace ':9092' with the actual address and port of your Kafka server.\n",
        "\n",
        "value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
        "This defines how to convert (deserialize) the incoming messages from Kafka back into Python objects:\n",
        "\n",
        "lambda x: loads(x.decode('utf-8')): This is a function that takes a message (x), decodes it from UTF-8 format, and then converts it from JSON format back into a Python dictionary using loads.\n",
        "'''"
      ],
      "metadata": {
        "id": "7_m4YOmziFbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = S3FileSystem()   # s3 = S3FileSystem(): Creates an object to work with Amazon S3 storage, enabling file operations like reading from and writing to S3 buckets"
      ],
      "metadata": {
        "id": "7pysQ4pUhiO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for count, i in enumerate(consumer):    #  Loops through Kafka messages, getting an index (count) and the message (i).\n",
        "    with s3.open(\"s3://kafka-stock-market/stock_market_{}.json\".format(count), 'w') as file:    # Opens a new file in S3 for writing, with a name based on the index.\n",
        "        json.dump(i.value, file)    # Writes the Kafka message content as JSON to the S3 file"
      ],
      "metadata": {
        "id": "lRy0sA13hiRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5cfq1EdShiTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICZcgiwAhiVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BILiGJ06hiYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9PrtG1PhiaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BKNxJWwghidl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}